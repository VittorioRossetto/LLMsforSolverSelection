\documentclass[../../main]{subfiles}
\begin{document}

From the study explained in the course of \Cref{chap:expEval} some important insights emerged. First and foremost, the limitation over token per request and over token per context window is a problem that we only managed to work around with the techniques explained in \Cref{sec:expSetup}, before adding the use of a feature extractor (\Cref{sec:features}).

Another limitation in LLMs, as briefly explained in \Cref{sec:LLM}, is that they work only with statistic prediction, based on their previous knowledge, this approach clearly creates gaps when the LLM is questioned with a completely new problem. To surpass this limitation, we needed a way to standardize all the problems so that an LLM can evaluate directly based on the problem characteristics.

The use of structured input, extracted from the problem scripts, proved to be a viable solution to these problems, showing the highest score yet. But while the results obtained with \texttt{mzn2feat} proved to be consistent, we still believed there could be a better way to formalize MiniZinc problems for an LLM.

The development of \texttt{fzn2nl}~\cite{fzn2nl}, was born from the idea to create a \texttt{FlatZinc} parser, that taken a file as input, produces a deterministic natural language description of the problem, exposing its main characteristics, in a way that is best suitable for an LLM. 

\dots Descrizione per sezione \dots

\section{Motivation}

Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. 
Parametric knowledge of LLMs remains mostly static~\cite{FTLLMallucination} after the pre-training stage, whereas knowledge in the world continues to change. 

Even within the pre-training data, knowledge from recent years can conflict earlier knowledge. But, the auto-regressive training objective biases LLMs toward surfacing more frequent but not necessarily recent knowledge~\cite{knowledgeCutoffsLLM}. This problem surfaces primarily when the LLM is confronted with more indirect questions, given that updates in real world lead to nuanced and complex changings in model knowledge (\Cref{fig:indirectProbing})~\cite{MemorizationVSReasoning}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{IndirectProbing.png}
    \caption{Example of LLM that is pre-trained on updated knowledge surfacing updates in the direct probing but failing under indirect probing settings (example image was taken from~\cite{MemorizationVSReasoning}).}
    \label{fig:indirectProbing}
\end{figure}

In our setup, the LLM acts as the reasoning brain behind solver selection, so we need a way to give it a description of the problem that is as direct as possible, to actively avoid favoring ``old'' problems. 

The most simple solution under those constraints is to create a deterministic description of each problem, providing enough knowledge for reasonable choice while hiding most recognizable traits of well-known problems (retrievable from the code).

Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, they often struggle with complex reasoning tasks and are prone to hallucination. And while structured data, rich in logical and relational information, has the potential to enhance the reasoning abilities of LLMs. Still, its integration poses a challenge due to the risk of overwhelming LLMs with excessive tokens and irrelevant context information~\cite{LLMwStructData}.

Those information lead to the development of \texttt{fzn2nl}, a parser that taken as input a \texttt{FlatZinc} file, directly translates its parameters to a structured natural language description of the given problem.

\section{FlatZinc Compiler}
\label{sec:parserCompilation}
In order to compile FlatZinc files, it is necessary to use the ``MiniZinc Compiler''~\cite{mznCompiler}. 

Constraint problems formulated in MiniZinc are solved by translating them to a simpler, solver-specific subset of MiniZinc, called FlatZinc, with the use of the ``MiniZinc Compiler''~\cite{mznCompiler}.
The complexities in the translation arise from the need to simultaneously (a) unroll array comprehensions (and other loops), (b) replace predicate and function applications by their body, and (c) flatten expressions.

% Once common subexpression elimination (CSE) is taken into account, it's not feasible to perform these separately. In order to have names for common subexpressions, expressions need to be flattened. And in order to take advantage of functions, for CSE we cannot replace predicate and function applications without flattening to generate these names. And without replacing predicate and function application by their body we are unable to see all the loops to unroll.

The translation algorithm generates a flat model equivalent to the original model as a global set of constraints. The translation uses full reification to create the model~\cite{MiniZincwFunctions}. Common subexpression elimination is implemented using a technique similar to hash-consing in Lisp~\cite{LISP}. 

\subsection{FlatZinc Limitation}
\label{sec:fznLimits}
As previously stated, Flatzinc is solver-specific, which is a clear limitation in our use case. FlatZinc solvers specify the set of global constraints they handle through dedicated propagators. When a given global constraint (e.g., \texttt{alldifferent} or \texttt{circuit}) is supported natively by the target solver, it is preserved in the compiled FlatZinc model and processed using the solver's specialized filtering algorithms. Otherwise, the MiniZinc compiler replaces the global constraint with an equivalent decomposition into more primitive constraints expressed in the solver's supported constraint language.

As a consequence, constraint programming solvers such as Gecode~\cite{Gecode} typically retain many global constraints in their high-level form, exploiting dedicated propagation mechanisms. In contrast, solvers based on alternative paradigms, such as linear programming or mixed-integer linear programming (e.g., Gurobi~\cite{Gurobi}), require these constraints to be reformulated as sets of linear constraints, thereby losing the original global structure in favor of a representation compatible with their underlying solving technology~\cite{CPvsMILPsolvers}.

Given that, by translating a FlatZinc file with a specific solver, and serving its analysis to an LLM, we are indirectly pushing the LLM towards the solver used for translation, or one of the same category, therefore invalidating the reasoning process.

\subsection{Custom Compiler}
In order to solve the problem of solver-specific translation, we had to modify the MiniZinc compiler, to produce a pseudo-FlatZinc that is not directly dependant on solvers, at the cost of not being actually solvable.

As stated in \Cref{sec:fznLimits}, the difference between the use of one solver over the other is relative to the different propagation of global constraint. 

So, in order to eliminate this distinction, we simply eliminated propagation as a whole, changing MiniZinc compiler code to completely avoid the substitution of predicates by their body. For example:

\begin{lstlisting}[style=minizincstyle]
include "arg_max.mzn";
predicate fzn_maximum_arg_bool_opt(array [int] of var opt bool: x, var int: z) =
    let {
        array[index_set(x)] of var 0..2: dx = array1d(index_set(x), [(xi + 1) default 0 | xi in x]);
    } in maximum_arg(dx, z);
\end{lstlisting}

Became:

\begin{lstlisting}[style=minizincstyle]
include "arg_max.mzn";
predicate fzn_maximum_arg_bool_opt(array [int] of var opt bool: x, var int: z);  
\end{lstlisting}

Clearly, with this change, the FlatZinc is no longer directly solvable, but since our only need is for it to be non solver-specific and understandable when ``explained'' to an LLM, this is not a problem.

For the sake of explanation, we'll use the job shop problem as an example, as defined in \Cref{fig:minizincModelEx} and \Cref{fig:minizincDataEx}. Using the compiler as defined in this section, the resulting FlatZinc is the one displayed in \Cref{fig:fznExample}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fznExample.png}
    \caption{FlatZinc resulting from the compilation of the program in \Cref{fig:minizincModelEx} with the data in \Cref{fig:minizincDataEx}, using the compiler defined in \Cref{sec:parserCompilation}}
    \label{fig:fznExample}
\end{figure}

\section{System Functioning}
\label{sec:fzn2nlArch}
The production of a viable natural language description with \texttt{fzn2nl} is made through a single-pass pipeline: first of all, the program takes a FlatZinc file as input, then this file is parsed, in order to extract variables, arrays, constraints, solve directive, and search annotations (when present) into an internal model object. The program then computes descriptive statistics on both variables and constraints, and reconstruct the objective structure. All of the parsed artifacts are mapped into readable sentences and grouped summaries. Finally, the system produces a report as output, consisting of problem description, variables description and constraint description.

\subsection{Parser}
The architecture of the implemented parser for this project, follows a modular design, separating data representation from syntactic extraction and higher-level semantic enrichment. 

\subsubsection{Core Data Structures}

The central abstraction is the \texttt{FlatZincModel} class, which acts as a container for all extracted elements. It maintains dictionaries for scalar variables and arrays, a list of constraints, a map of definition dependencies, and metadata concerning the problem type, objective function, and search annotation.

Scalar variables are stored in a dictionary indexed by name. Each entry records the declared type (integer or Boolean), an optional domain, and an origin flag distinguishing user-declared variables from compiler-introduced ones. Domains are represented by an immutable \texttt{Domain} data class containing minimum and maximum bounds, together with a derived mean value. 

Arrays are stored separately from scalar variables. For each array, the parser records element type, whether elements are decision variables, index range length (when statically inferable), and the list of items when explicitly provided.

Constraints are stored as structured dictionaries containing the constraint type, argument string, annotation string, reconstructed textual rendering, and any variables declared via \texttt{defines\_var} annotations, preserving sufficient information for both textual reporting and dependency reconstruction.

\subsubsection{Parsing Strategy}

The parser operates in a single pass over the file contents, using regular expressions for high-level pattern detection and auxiliary scanning procedures for balanced parenthesis handling. The overall workflow can be described as follows.

First, scalar variable declarations are extracted using line-anchored regular expressions. The parser distinguishes between standard integer and Boolean declarations and explicit domain specifications, such as interval domains (e.g., \texttt{1..10}) or set domains (e.g., \texttt{\{1,3,5\}}). Domains are interpreted conservatively: when a set is provided, only its minimum and maximum values are retained.

Second, array declarations are parsed independently. The index range is analyzed to determine the array length when specified as a closed interval. If an initializer is present, its elements are collected in textual form. This design avoids premature semantic interpretation while preserving structural information.

Third, constraints are parsed using a hybrid approach. A regular expression identifies occurrences of the keyword \texttt{constraint} followed by a constraint symbol. Since FlatZinc arguments may contain nested parentheses, a dedicated balanced-call extractor is used to recover the full argument list safely. Trailing annotations are preserved, and any \texttt{defines\_var} markers are recorded. A secondary pass constructs a definition map from variables to the constraints that define them.

Finally, the \texttt{solve} statement is parsed to determine whether the problem is a satisfaction or optimization instance. If a search annotation is present, a recursive routine interprets common patterns such as \texttt{int\_search} and \texttt{seq\_search}. The resulting structure encodes variable selection strategy, value selection strategy, completeness mode, and, in the case of sequential search, the list of phases.

A heuristic mechanism is implemented to distinguish user-defined variables from those introduced during compilation. The detection combines name-based patterns (e.g., identifiers containing \texttt{INTRODUCED}) with annotation-based indicators (e.g., \texttt{is\_defined\_var}). Although not formally guaranteed to be complete, this approach is robust across common FlatZinc toolchains and enables analyses restricted to original model variables.

Beyond syntactic parsing, the architecture includes lightweight analytical utilities. For example, a degree computation function estimates, for each scalar variable, the number of constraints in which it appears. This is achieved by tokenizing constraint argument strings and counting occurrences of known variable identifiers. While approximate, this metric provides a structural indicator of variable centrality in the constraint graph.

The architecture deliberately avoids constructing a full abstract syntax tree of the FlatZinc language. Instead, it adopts a pragmatic intermediate representation that captures structural information sufficient for meta-analysis and feature extraction. Balanced-parenthesis scanning is used selectively to overcome the limitations of purely regular-expression-based parsing, particularly in constraint arguments and search annotations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Example of Parser Execution on a FlatZinc Instance}
To better explain the functioning of the parser, we'll explain how it would behave when given the FlatZinc script defined in \Cref{fig:fznExample} as input.
The parser first processes scalar variable declarations. The following integer decision variables are identified in the lines 1-6:

\begin{itemize}
\item \texttt{X\_INTRODUCED\_1\_} with domain $[0,7]$,
\item \texttt{X\_INTRODUCED\_2\_} with domain $[2,9]$,
\item \texttt{X\_INTRODUCED\_3\_} with domain $[0,7]$,
\item \texttt{X\_INTRODUCED\_4\_} with domain $[3,10]$,
\item \texttt{end} with domain $[7,14]$.
\end{itemize}

In addition, four Boolean variables are declared in the lines 7-10:

\[
\texttt{X\_INTRODUCED\_10\_},\;
\texttt{X\_INTRODUCED\_12\_},\;
\texttt{X\_INTRODUCED\_14\_},\;
\texttt{X\_INTRODUCED\_15\_}.
\]

All variables whose identifiers contain the substring \texttt{INTRODUCED} are classified as compiler-introduced by the heuristic detection mechanism. The Boolean variables are also annotated with \texttt{var\_is\_introduced} and \texttt{is\_defined\_var}, which reinforces this classification. The variable \texttt{end}, although not matching the naming pattern, is treated as user-defined since no introduction annotation is present.

Each integer domain expressed as an interval is converted into a \texttt{Domain} object storing the minimum and maximum bounds. Boolean variables are internally represented with domain $[0,1]$.

Two array declarations are parsed.

The first, in line 1,
\[
\texttt{array [1..2] of int: X\_INTRODUCED\_8\_ = [1,-1];}
\]
is recognized as a fixed integer array of length $2$. Its elements are stored as the literals $1$ and $-1$. Since its name matches the introduction pattern, it is classified as compiler-introduced.

The second, in line 11,
\[
\texttt{array [1..4] of var int: X\_INTRODUCED\_0\_ = [...]},
\]
is recognized as an array of integer decision variables of length $4$. The stored items correspond to the scalar variables
\[
\texttt{X\_INTRODUCED\_1\_},\;
\texttt{X\_INTRODUCED\_2\_},\;
\texttt{X\_INTRODUCED\_3\_},\;
\texttt{X\_INTRODUCED\_4\_}.
\]
The array is marked as containing decision variables (\texttt{is\_var = true}) and as compiler-introduced.

The parser then extracts all constraint declarations. Each constraint is represented internally by its type, argument string, annotations, and any variables declared through \texttt{defines\_var}.

Four linear inequality constraints of type \texttt{int\_lin\_le} are identified, in lines 12,13,15 and 16. Each uses the coefficient array \texttt{X\_INTRODUCED\_8\_} and a pair of integer variables. For example,
\[
\texttt{int\_lin\_le(X\_INTRODUCED\_8\_, [X\_INTRODUCED\_1\_, X\_INTRODUCED\_2\_], -2)}
\]

would result as:
\[
\begin{aligned}
\{\\
\quad &\texttt{"type"}:\; \texttt{"int\_lin\_le"},\\
\quad &\texttt{"args"}:\; \parbox[t]{0.72\textwidth}{\ttfamily%
"X\_INTRODUCED\_8\_,\newline%
[X\_INTRODUCED\_1\_, X\_INTRODUCED\_2\_],\newline%
-2"},\\
\quad &\texttt{"ann"}:\; \texttt{""},\;\text{\scriptsize (empty string, since no trailing annotation is present)}\\
\quad &\texttt{"text"}:\; \parbox[t]{0.72\textwidth}{\ttfamily%
"int\_lin\_le(X\_INTRODUCED\_8\_,\newline%
[X\_INTRODUCED\_1\_, X\_INTRODUCED\_2\_],\newline%
-2)"},\\
\quad &\texttt{"defines"}:\; [\,]\;\text{\scriptsize (empty since the constraint does not contain \texttt{defines\_var} annotation)}\\
\}
\end{aligned}
\]

Two \texttt{bool\_clause} constraints are parsed, respectively at lines 14 and 17. Each clause contains a list of Boolean variables and an empty negative literal list, corresponding to a disjunction over the given positive literals.

Four reified linear constraints of type \texttt{int\_lin\_le\_reif} are also extracted, at lines 18-21. Each includes a trailing annotation of the form
\[
\texttt{:: defines\_var(X\_INTRODUCED\_k\_)}.
\]
During post-processing, the parser builds a definition map associating each of the Boolean variables
\[
\texttt{X\_INTRODUCED\_10\_},\;
\texttt{X\_INTRODUCED\_12\_},\;
\texttt{X\_INTRODUCED\_14\_},\;
\texttt{X\_INTRODUCED\_15\_}
\]
with the corresponding reified constraint that defines it.

The final statement,
\[
\texttt{solve minimize end;}
\]
is parsed as an optimization directive. The model is classified as a minimization problem, with objective variable \texttt{end}. No search annotation is provided; therefore, the \texttt{search} field in the internal representation remains \texttt{None}.

% To sum up everything, after parsing, the internal model contains:

% \begin{itemize}
% \item nine scalar decision variables (five integer and four Boolean),
% \item two arrays (one constant integer array and one array of decision variables),
% \item ten constraints,
% \item four entries in the definition map linking Boolean variables to reified linear constraints,
% \item problem type set to \texttt{minimize} with objective variable \texttt{end}.
% \end{itemize}

\subsection{Mapping to Natural Language}

Once the FlatZinc instance has been parsed into the internal \texttt{FlatZincModel} representation, \texttt{fzn2nl} performs a deterministic mapping step that turns the extracted structures into a natural language report.

The mapping stage is implemented as a set of specialized formatters, each targeting a distinct part of the FlatZinc model. Rather than generating free-form text, each formatter follows fixed templates and uses controlled vocabularies defined in dedicated mapping tables.

\paragraph{Constraint descriptions and categorization}
FlatZinc constraints are reported primarily by type. For each constraint type, the mapper aggregates the total number of occurrences in the instance, and the constraint arity. Since FlatZinc arguments can contain arrays, the arity estimate expands known arrays of decision variables into their element variables when possible, and otherwise falls back to array length when statically inferable.

To attach a human-readable explanation to each constraint type, \texttt{fzn2nl} uses a layered description lookup, adding a description corpus extracted from MiniZinc/FlatZinc documentation and  an optional categorized version of the same corpus, which allows constraint types to be grouped under conceptual families such as scheduling, graph, counting, and packing constraints.

The resolver is robust to common naming variations introduced by compilation and library predicates. For example, when a constraint is prefixed by \texttt{fzn\_} or carries suffixes such as \texttt{\_reif}, the resolver progressively strips prefix and trailing tokens to find a meaningful base description.

\paragraph{Search-annotation mapping}
When the \texttt{solve} item includes an explicit search annotation, the parser extracts it into a structured object (e.g., \texttt{int\_search} or \texttt{seq\_search}). The natural language mapper then translates the search parameters using controlled vocabularies. For sequential searches, the report enumerates phases and describes each phase with the same template.

When the internal model object is available, \texttt{fzn2nl} enrich search descriptions, mentioning a domain range and providing simple structural context such as whether the target is a scalar or an array and how many elements are involved.

\paragraph{Objective-function reconstruction and abstraction}
FlatZinc explicitly specifies only the optimization direction (minimize or maximize) and an objective variable. The high-level expression for that variable is typically not present as a single expression, but it can be reconstructed in a best-effort way using compilation annotations such as \texttt{defines\_var}. \texttt{fzn2nl} builds a shallow expression tree by following the chain of defining constraints for the objective variable and rendering common arithmetic constructs (e.g., \texttt{int\_plus}, \texttt{int\_minus}, \dots). If reconstruction is not possible, the report falls back to describing the objective variable alone.

The objective expression is additionally rewritten into an abstract form, by replacing variable names with placeholders (e.g., $a,b,c,\dots$). This preserves the algebraic structure of the objective while hiding FlatZinc-specific naming patterns (such as \texttt{X\_INTRODUCED\_k\_}). The abstract expression is also length-limited to ensure the report remains concise.

\paragraph{Variable statistics}
Finally, \texttt{fzn2nl} maps variables and arrays into summary statistics. The mapper separates user-declared and compiler-introduced variables, distinguishes between integer and Boolean variables, and summarizes known finite integer domains via simple bucketed distributions. Arrays of decision variables are treated as collections of element variables when their elements are explicitly listed; otherwise the mapper falls back to counting by array length when available.

Following the example, the resulting natural language description extracted from \Cref{fig:fznExample}, is:

Problem:
  This is a minimization problem. The objective is to minimize an objective variable with domain [7, 14] (size 8, mean 10.50) and degree 2. The objective function is in the form: minimize a. No explicit search strategy is specified.

Variables:
The model contains 9 variables (5 integer, 4 Boolean): 8 compiler-introduced and 1 user-introduced.
Among 5 integer variables with known finite domains, 100.0\% have domain size in [8, 8] (avg size 8.00).

Constraints:
Bool FlatZinc builtins: 1 type, 2 constraints (avg arity 2.00)\\
\quad bool\_clause: 2 constraints with average arity 2.00 
(constrains $\bigvee_i as[i] \lor \bigvee_j \lnot bs[j]$)

Integer FlatZinc builtins: 2 types, 8 constraints (avg arity 2.50)\\
\quad int\_lin\_le: 4 constraints with average arity 2.00 
(constrains $\sum$ as[i] * bs[i] $\leq$ c)

\quad int\_lin\_le\_reif: 4 constraints with average arity 3.00 
(constrains $r \leftrightarrow$ ($\sum$ as[i] * bs[i] $\leq$ c))

\section{Testing and Results}
To understand the potential of \texttt{fzn2nl}, we tested the LLM while giving only the generated natural language report, and the report combined with solver description, as described in \Cref{sec:sDesc}. The two configurations were then tested over the significative solver set, with all the different sampling temperatures defined in \Cref{sec:sampTemp}, results are displayed in \Cref{tab:combinedFznTemp}

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Variant & Temperature & Single Score & Parallel Score & Closed Gap \\ \hline
            fzn2nl & 0.7 & 52.843 & 73.601 & -0.318 \\ 
            fzn2nl & 0.2 & 52.002 & 75.143 & -0.354 \\ 
            fzn2nl + Solver Description & 0.7 & 51.305 & 73.374 & -0.384 \\ 
            fzn2nl & 0.0 & 50.761 & 72.702 & -0.407 \\ 
            fzn2nl + Solver Description & 0.3 & 50.576 & 73.893 & -0.415 \\ 
            fzn2nl + Solver Description & 0.8 & 49.826 & 73.545 & -0.447 \\ 
            fzn2nl & 0.8 & 48.582 & 74.979 & -0.500 \\ 
            fzn2nl & 0.3 & 49.423 & 71.380 & -0.464 \\ 
            fzn2nl + Solver Description & 0.0 & 49.098 & 72.374 & -0.478 \\ 
            fzn2nl + Solver Description & 0.2 & 49.076 & 73.624 & -0.479 \\ \hline
        \end{tabular}
    }
    \caption{Sampling-temperature sweep on the variants including \texttt{fzn2nl} output, using \texttt{gpt-oss-120b}. Score columns are calculated as in \Cref{tab:combinedAllLLM}.}
    \label{tab:combinedFznTemp}
\end{table}

\dots discussione risultati \dots

\subsection{Performance with GPT-5.2}
To reach further understanding on the tool potential, and on LLMs in general, we also tried testing with a state of the art LLM chatbot: GPT-5.2\cite{gpt52}.
A model claimed to set a new state of the art across many benchmarks, including GDPval~\cite{GDPval}, where it outperforms industry professionals at well-specified knowledge work tasks spanning 44 occupations.

Overall, GPT-5.2 is said to bring significant improvements in general intelligence, long-context understanding, agentic tool-calling, and vision, making it better at executing complex, real-world tasks end-to-end than any previous model.

The need for (even partial) testing on a current state-of-the-art LLM, is to understand how big of an impact, the use of limited free-tier LLMs, is making in our testing.

However, testing on this model requires the use of a chatbot~\cite{chatGPT}, where we manually insert requests, and retrieve the answers, other than that the use of this model has usage limitations~\cite{gpt52Limits}, putting a cap to the number of requests that could be performed over a day.

Given those limits to the LLM usage, we opted for a limited testing setup: instead of prompting all of the instances reports, we only used one for problem, extracting from an arbitrary instance. Then we used the the suggestion as if it was given for all the instances of that given problem.

\dots tabella \dots

As displayed in \dots, the usage of GPT-5.2 in this limited setup, hasn't displayed any significative improvement in ``Single Score'', and consequently ``Closed Gap''. The calculated score is lower then 3 of the previously tested variants, displayed in \Cref{tab:combinedFznTemp}, and, consequently, behind the SBS score.
\end{document}