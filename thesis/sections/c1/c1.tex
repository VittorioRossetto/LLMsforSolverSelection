\documentclass[../../main]{subfiles}
\begin{document}

\section{Methodology}
In this section, the focus is on outlining the foundational decisions required to establish an initial benchmark. This benchmark serves as the basis for refining subsequent experiments and assessing both the current capabilities and future potential of the solver.

The section is structured into four subsections, each addressing a key preliminary choice. The first subsection discusses the selection of large language models used as candidates for the agentic component. The second details the initial prompt-engineering strategy needed to define a clean, consistent prompt format for evaluation. The third presents the rationale behind the selection of benchmark problems used to test model performance. The fifth explains the metrics adopted to evaluate how effectively each model could operate as a meta-solver. The final subsection explains the pre-processing methods adopted the make automatic testing possible.

\subsection{Provider Choice} 
To build the proposed Agentic Solver (AS), the first requirement is the
availability of an LLM capable of orchestrating the system and acting as the
agent. Given the limited computational resources available during the testing
phase, we had to rely on externally hosted LLMs accessed through
usage-based APIs. The selection prioritized generous free tiers, permissive rate
limits, and straightforward integration. This led to the choice of the
following providers:
\begin{itemize}
    \item \texttt{Gemini API v1}\cite{GeminiAPI}, offered by Google DeepMind. Gemini is a family of large language models with multiple sizes and capabilities. This provider selected for its strong reasoning abilities, robust tool-use features, and overall high-quality text generation. For the purpose of this research, the version v1\cite{GeminiAPIver} was prefered as it is more stable and our only concern is its text generation capability.

    \item \texttt{Groq API}\cite{GroqAPI}, provided by Groq. Groq offers
    high-performance inference solutions through its specialized hardware
    architecture. The Groq API exposes a selection of LLMs through a simple and
    lightweight interface, enabling fast and low-latency experimentation.
\end{itemize}

Both APIs were selected for their ease of use, flexibility, overall performance,
and, critically, their comparatively generous rate limits relative to competing
services, in \Cref{tab:rate_limits_gemini} and \Cref{tab:rate_limits_groq} are displayed rate limits of both APIs. From the leftmost column of the table, there are: Model containing the names of each one of the available LLMs (for the purpose of this paper, only text generation models were selected), moving to the right \textit{RPM} contains the maximum number of requests in a minute, \textit{RPD} contains the maximum number of requests per day, \textit{TPM} contains the maximum number of requested tokens per minute, and finally \textit{TPD} contains the maximum number of tokens per day.

\begin{table}[ht]
    \centering
    \small % reduce font size for wide tables
    \label{tab:rate_limits_groq}
    \begin{tabular}{@{} >{\raggedright\arraybackslash}p{6cm} r r r r @{}}
        \toprule
        Model & RPM & RPD & TPM & TPD \\
        \midrule
        allam-2-7b & 30 & 7000 & 6000 & 500000 \\
        deepseek-r1-distill-llama-70b & 30 & 1000 & 6000 & 100000 \\
        gemma2-9b-it & 30 & 14400 & 15000 & 500000 \\
        groq/compound & 30 & 250 & 70000 & -- \\
        groq/compound-mini & 30 & 250 & 70000 & -- \\
        llama-3.1-8b-instant & 30 & 14400 & 6000 & 500000 \\
        llama-3.3-70b-versatile & 30 & 1000 & 12000 & 100000 \\
        meta-llama/llama-4-maverick-17b-128e-instruct & 30 & 1000 & 6000 & 500000 \\
        meta-llama/llama-4-scout-17b-16e-instruct & 30 & 1000 & 30000 & 500000 \\
        meta-llama/llama-guard-4-12b & 30 & 14400 & 15000 & 500000 \\
        meta-llama/llama-prompt-guard-2-22m & 30 & 14400 & 15000 & 500000 \\
        meta-llama/llama-prompt-guard-2-86m & 30 & 14400 & 15000 & 500000 \\
        moonshotai/kimi-k2-instruct & 60 & 1000 & 10000 & 300000 \\
        moonshotai/kimi-k2-instruct-0905 & 60 & 1000 & 10000 & 300000 \\
        openai/gpt-oss-120b & 30 & 1000 & 8000 & 200000 \\
        openai/gpt-oss-20b & 30 & 1000 & 8000 & 200000 \\
        playai-tts & 10 & 100 & 1200 & 3600 \\
        playai-tts-arabic & 10 & 100 & 1200 & 3600 \\
        qwen/qwen3-32b & 60 & 1000 & 6000 & 500000 \\
        \bottomrule
    \end{tabular}
    \caption{Rate limits - Groq models\cite{GroqRL}: \\This table shows all the offered models from Groq API, in leftmost column and each relative rate limit.}
\end{table}

\begin{table}[ht]
    \centering
    \small
    \label{tab:rate_limits_gemini}
    \begin{tabular}{@{} >{\raggedright\arraybackslash}p{6cm} r r r r @{}}
        \toprule
        Model & RPM & RPD & TPM & TPD \\
        \midrule
        gemini-2.5-pro & 5 & 100 & 250000 & -- \\
        gemini-2.5-flash & 10 & 250 & 250000 & -- \\
        gemini-2.5-flash-lite & 15 & 1000 & 250000 & -- \\
        gemini-2.0-flash & 15 & 200 & 1000000 & -- \\
        gemini-2.0-flash-lite & 30 & 200 & 1000000 & -- \\
        \bottomrule
    \end{tabular}
    \caption{Rate limits - Gemini models\cite{GeminiRL}\\This table shows all the offered models from Gemini API, in leftmost column and each relative rate limit.}
\end{table}

\subsection{Large Language Models Selection}

Both providers offer a broad set of LLMs with varying capabilities and constraints, so an initial filtering step was required. Several options were excluded immediately because they are not designed for text generation, which is essential for the proposed AS. In particular, \texttt{playai-tts} and \texttt{playai-tts-arabic} are text-to-speech LLMs available only on Groq's platform and therefore unsuitable for remote testing.

Additional LLMs were removed because they are currently decommissioned or unavailable: \texttt{deepseek-r1-distill-llama-70b}, \texttt{gemini-2.0-flash-lite}, and \texttt{gemma2-9b-it}.

Two more LLMs were excluded due to insufficient context window size. Although their rate limits were acceptable, their token capacity was too small to accommodate even a single full MiniZinc model as input: \texttt{meta-llama/llama-prompt-guard-2-22m} and 

\noindent
\texttt{meta-llama/llama-prompt-guard-2-86m}.

Finally, \texttt{allam-2-7b} was removed because it failed to follow instructions consistently, often producing incomplete, inconsistent, or unreadable outputs.


After this filtering stage, 18 LLMs remained as a stable base for the
evaluation phase.

\subsection{Prompt General Structure}
\label{sec:promptStruct}
To determine which LLM would be best suited for building an AS, it was necessary to design a consistent prompt format to query each model.
The primary objective was to define a structure that was as short and clean as possible, for two main
reasons:
\begin{itemize}
    \item Minimize prompt-induced bias: A highly descriptive or too long and complex prompt could influence LLMs negatively. As we could encounter problems as "context rot"\cite{ContextRot} - a progressive decay in accuracy as prompts grow longer.
    \item Reduce token usage: Since the testing setup depends on API limits, keeping the prompt compact minimizes token consumption.
\end{itemize}

\subsubsection{Output Structure}
Ensuring a standardized output format was equally important:
Automated testing requires that model outputs follow a strict and predictable format. Any deviation introduces ambiguity during parsing and prevents reliable extraction of solver selections. Maintaining this structure is therefore essential to ensure consistent and fully automated evaluation.

Large or verbose responses also impose practical limitations on the available context window. Because each message contributes to the total token count, excessively long outputs reduce the room available for subsequent turns and larger prompts.

For these reasons, the output format was fixed as an array of three strings:
\[
["1^{st}\text{Solver}",\ "2^{nd}\text{Solver}",\ "3^{rd}\text{Solver}"]
\]

Selecting the top three solvers enables two forms of evaluation:
\begin{itemize}
    \item Single-solver evaluation: Measures whether the solver chosen by the LLM is the single best solver for the given instance. If it is not, the evaluation can quantify how close its performance is to the optimal solver.
    \item Parallel-solver evaluation: Measures the effectiveness of running the top three solvers selected by the LLM in parallel. The best result among the three is considered, allowing assessment of whether any of them corresponds to the single best solver for the instance, or, if not, how close the best among the three comes to the optimal performance.
\end{itemize}
The metrics used for these evaluations will be detailed in subsection~\ref{sec:metrics}.

After all of this considerations, the resulting prompt structure is the one displayed in \Cref{fig:promptEx} 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{../../PromptExample.png}
    \caption{Example of prompt}
    \label{fig:promptEx}
\end{figure}

\subsection{Problem Selection}
\label{sec:probSelection}
A crucial component of the testing pipeline is the problem selection.
Consistent and meaningful evaluation requires a set of benchmark problems that
are reliable, diverse, and representative of real solver behavior. To meet
these requirements, the problem set should satisfy the following criteria:
\begin{itemize}
    \item Extensive prior testing: The problems must be validated and
    associated with reliable solver performance data, preferably obtained from
    recent evaluations of state-of-the-art solvers.
    \item Diversity: The set must include a varied mix of
    problem types-combinatorial problems, real-world applications,
    and puzzle-like tasks-covering all major categories: Maximization, Minimization and Satisfaction.

    This ensures that LLM performance can be assessed across different solving
    paradigms.
    \item Complexity: The problems must be sufficiently challenging so
    that solver selection is non-trivial and the LLM's reasoning abilities are
    meaningfully tested.
\end{itemize}
Following these criteria, the selected benchmark was the problem set from the
\textit{MiniZinc Challenge 2025}\cite{PhilMznChallenge}\cite{MznProblems}\cite{MznResults25}. These problems are specifically curated
to benchmark the strongest solvers of the year and therefore represent an ideal
test bed for evaluating the proposed Agentic Solver.

The problem set contains twenty problems: 1 satisfaction problem, 3 maximization problems, 16 minimization problems.

Each problem is a combination of a \texttt{.mzn} file containing the Minizinc\cite{MiniZinc} model made of the high-level description of the problem (variables, constraints and objective function). Every problem also is also accompanied by five corresponding data instances each of them contained either in a \texttt{.dzn} or a \texttt{.json} file containing specific parameters and constants, yielding a total of
100 testable, diverse, and complex scenarios.

\subsection{Test Metrics}
\label{sec:metrics}
In order to actually evaluate model performance, it is necessary to chose a standard metric for answer evaluation, other than that, it is necessary to have a metric to evaluate how an AS controlled by the given LLM would perform against the current Single Best Solver (SBS).

Before analysing the evaluation metrics, we must first define the systems to which these metrics will be applied. Namely, the solvers. In our context, a solver is a program that takes as input the description of a computational problem in a given language and returns an observable outcome providing zero or more solutions for the given problem.  For example, for decision problems, the outcome may be simply "yes" or "no" while for optimization problems, we might be interested in the best solutions found along the search. An evaluation metric, or performance metric, is a function mapping the outcome of a solver on a given instance to a number representing "how good" the solver is on this instance. An evaluation metric is often not just defined by the output of the solver. Indeed, it can be influenced by other actors, such as the computational resources available, the problems on which we evaluate the solver, and the other solvers involved in the evaluation. For example, it is often unavoidable to set a \texttt{timeout} \(\tau\) on the solver's execution when there is no guarantee of termination in a reasonable amount of time (e.g. NP-hard problems). Timeouts make the evaluation feasible but inevitably couple the evaluation metric to the execution context. For this reason, the evaluation of a meta-solver should also consider the scenario that encompasses the solvers to evaluate, the instances used for the validation, and the timeout. Formally, at least for the purposes of this paper, we can define a scenario as a triple (\(\mathcal{I}, \mathcal{S},\tau\)), where: \(\mathcal{I}\) is a set of problem instances, \(\mathcal{S}\) is a set of individual solvers, \(\tau \in (0,+\infty) \) is a timeout such that the outcome of solvers \(s \in \mathcal{S}\) Solver instance \(i \in \mathcal{I}\) is always measured in the time interval [\(0,\tau\)].
Evaluating meta-solvers over heterogeneous scenarios (\(\mathcal{I}_1, \mathcal{S}_1,\tau_1\)), (\(\mathcal{I}_2, \mathcal{S}_2,\tau_2\)), \dots, is complicated by the fact that the sets of instances \(\mathcal{I}_k\), the sets of solvers \(\mathcal{S}_k\) and the timeouts \(\tau_k\) can be very different. And things could get even more complicated in scenarios including optimization problems.

For those objectives two separate metrics were chosen


\subsubsection{Metric for Solver Score}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{../../TableSolverMetricsExample.png}
    \caption{Solver performances example}
    \label{fig:solverperformaceex}
\end{figure}

We are now ready to associate to every instance $i$ and solver $s$ a weight that
quantitatively represents how good is $s$ when solving $i$ over time $T$. We
define the scoring value of $s$ (shortly, score) on the instance $i$ at a
given time $t$ as a function $\mathrm{score}_{\alpha,\beta}$\cite{evaluationMetaSolvers}\cite{PortfolioAproaches} defined as follows:

\[
\mathrm{score}_{\alpha,\beta}(s,i,t)=
\begin{cases}
0, &
    \text{if }\mathrm{sol}(s,i,t)=\mathrm{unk},
    \\[6pt]

1, &
    \text{if }\mathrm{sol}(s,i,t)\in\{\mathrm{opt},\mathrm{uns}\},
    \\[6pt]

\beta, &
    \begin{aligned}
    &\text{if }\mathrm{sol}(s,i,t)=\mathrm{sat}
    \\[-2pt]
    &\text{ and }\mathrm{MIN}(i)=\mathrm{MAX}(i),
    \end{aligned}
    \\[10pt]

\displaystyle
\max\!\left\{
  0,\;
  \beta-(\beta-\alpha)
  \frac{\mathrm{val}(s,i,t)-\mathrm{MIN}(i)}
       {\mathrm{MAX}(i)-\mathrm{MIN}(i)}
  \right\}, &
    \begin{aligned}
    &\text{if }\mathrm{sol}(s,i,t)=\mathrm{sat}
    \\[-2pt]
    &\text{and $i$ is a minimization problem},
    \end{aligned}
    \\[12pt]

\displaystyle
\max\!\left\{
  0,\;
  \alpha+(\beta-\alpha)
  \frac{\mathrm{val}(s,i,t)-\mathrm{MIN}(i)}
       {\mathrm{MAX}(i)-\mathrm{MIN}(i)}
  \right\}, &
    \begin{aligned}
    &\text{if }\mathrm{sol}(s,i,t)=\mathrm{sat}
    \\[-2pt]
    &\text{and $i$ is a maximization problem}.
    \end{aligned}
\end{cases}
\]


Here, $\mathrm{MIN}(i)$ and $\mathrm{MAX}(i)$ denote the minimal and maximal
objective function values found by any solver $s$ at the time limit $T$.

\medskip

As an example, consider the scenario in \Cref{fig:solverperformaceex} showing three different solvers
on the same minimization problem. Let $T=500$, $\alpha=0.25$, $\beta=0.75$.
Solver $s_1$ finds the optimal value (40), therefore it receives score $0.75$.
Solver $s_2$ finds the maximal value (50), hence score $0.25$. Solver $s_3$
does not find a solution in time, giving score $0$. If instead $T=800$,
the value of $s_1$ becomes $0.375$ and $s_3$ gets $0.75$. If $T=1000$,
since $s_3$ improves the objective to 10 (marked with a star in the figure),
it receives the highest score.

The parameter used for score calculation in testing are: $T=1200000$ (1200000ms = 20 minutes, which is the time limit used solver evaluation in the MiniZinc Challenge) $\alpha=0.25$ $\beta=0.75$.

\subsubsection{Closed Gap}
Once the evaluation metric for solver score has been defined, we also need a comparative metric after score calculation.
For this objective, we have chosen to use \textit{closed-gap}\cite{evaluationMetaSolvers} as the evaluation metric. 
Which is a relative and meta-solver-specific measure, adopted in the 2015 ICON and 2017 OASC \cite{lindauer2019}
challenges to handle the disparate nature of the scenarios, is the 
closed gap score. This metric assigns to a meta-solver a value in $(-\infty, 1]$ 
proportional to how much it closes the gap between the best individual solver available, 
or single best solver (SBS), and the virtual best solver (VBS), i.e., an 
oracle-like meta-solver always selecting the best individual solver. The closed gap is 
actually a "meta-metric", defined in terms of another evaluation metric $m$ to minimize, which in this case is the scoring metric defined earlier. 
Formally, if $(I, S, \tau)$ is a scenario then 
\[
m(i, \mathrm{VBS}, \tau) = \min \{ m(i, s, \tau) \mid s \in S \}
\quad\text{for each } i \in I,
\]
and 
\[
\mathrm{SBS} = \arg\min_{s \in S} \sum_{i \in I} m(i, s, \tau).
\]

With these definitions, \textit{Closed-gap} can be defined as follows: Let ($\mathcal{I}, S, \tau$) be a scenario and 
\[
m : \mathcal{I} \times \bigl(S \cup \{S, \mathrm{VBS}\}\bigr) \times [0,\tau] \to \mathbb{R}
\]
an evaluation metric to minimize for that scenario, where $S$ is a
meta-solver over the solvers of~$S$.
Let
\[
m_\sigma \;=\; \sum_{i\in\mathcal{I}} m(i,\sigma,\tau)
\qquad\text{for }\sigma\in\{S,\mathrm{SBS},\mathrm{VBS}\}.
\]
The closed gap of $S$ with respect to $m$ on that scenario is
\[
\frac{m_{\mathrm{SBS}} - m_S}{\,m_{\mathrm{SBS}} - m_{\mathrm{VBS}}\,}.
\]

\noindent

The assumption $m_{\mathrm{VBS}} > m_{\mathrm{SBS}}$ is required, i.e., no single-solver can be the $\mathrm{VBS}$ (otherwise, no algorithm selection would be needed, given that its objective is to reach the $\mathrm{VBS}$). Unlike other scores, the closed gap is designed specifically for meta-solvers. Applying it to individual solvers would
assign~0 to the $\mathrm{SBS}$ and a negative score to the remaining solvers,
proportional to their performance difference with respect to the $\mathrm{SBS}$ and
the gap $m_{\mathrm{SBS}} - m_{\mathrm{VBS}}$, which makes little sense for
individual solvers, as it wouldn't reflect their actual performance overall. 

\subsection{Experiment Setup}
\label{sec:expSetup}
We have defined both the structure of the queries posed to the LLMs (\Cref{sec:probSelection}) and the way in which these queries are formulated (\Cref{sec:promptStruct}). The remaining challenge is to evaluate them automatically over the full set of selected instances.
To this end, we designed an automated testing pipeline that parallelizes execution by assigning one thread per LLM. For each model, requests are issued sequentially, with five requests per problem, each containing a MiniZinc model and a single instance encoded as shown in \Cref{fig:promptEx}.

Despite preliminary prompt engineering and model filtering, several MiniZinc models, particularly their associated data files, still exceed the providers' rate limits. Since these limits are strict, additional mechanisms were required to prevent limit violations while still allowing evaluation over the complete instance set.

\subsubsection{Script Manipulation}

The most direct way to address oversized requests is to reduce their length. As the prompt itself was already minimal, this required direct manipulation of the MiniZinc model (\texttt{.mzn}) and data files.

A first step consisted in removing all non-essential elements, such as comments (starting with \texttt{\%}\cite{MiniZinc}), tabs, and unnecessary whitespace. While this helps reduce token usage and standardizes script formatting, it is insufficient on its own. The main contributor to token overflow is the presence of large data arrays, which not only increase message length but may also pollute the context, "distracting" the LLM from the most relevant information\cite{LLMdistract}.

To mitigate this issue, data arrays were truncated to a fixed maximum length of 30 elements, with an inline comment indicating the original size:
\[ 
[e_1,e_2,\dots,e_{30} \texttt{// array too long to display, dimensions: (150)} ]
\]
While effective for simple arrays of scalar values, this approach does not account for the complexity of individual elements and performs poorly on more structured data. For this reason, a second truncation mechanism was introduced based on raw character length. Arrays exceeding 90 characters were truncated accordingly, using the same annotation to preserve information about the original size. 

\subsubsection{Custom Delays}
Since each experiment involves multiple problems and multiple sequential requests per LLM, rate limits can still be exceeded even when individual requests are within bounds. To handle this, custom delays were introduced into the experiment orchestration logic.

When an error message is received, for example:

\begin{lstlisting}
Error code: 413 - Request too large for model `openai/gpt-oss-120b` in organization `org_01k9qqesvte4d9h5jnhmzvbmy4` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Requested 8939, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing
\end{lstlisting}

\begin{lstlisting}
Error code: 429 - Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01k9qqesvte4d9h5jnhmzvbmy4` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 193047, Requested 10632. Please try again in 26m29.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing
\end{lstlisting}

Its code is inspected. Errors $413$ and $429$ indicate that a rate limit has been exceeded. The error message is then parsed to identify the specific limit involved. If the limit concerns tokens per minute (TPM) or requests per minute (RPM), the system pauses execution for 60 seconds before retrying. If the exceeded limit is tokens per day (TPD) or requests per day (RPD), the message is further analyzed to extract the cooldown duration, typically expressed in the form $XXh,XXm,XX.XXs$ where $h$ stands for hours, $m$ for minutes and $s$ for seconds. The required delay is then  computed from this value, after which the request is retried.

% WebApp per richieste singole(?)
% primi test con tutti gli LLM senza manipolare script, dicendo alla fine che da qua in poi si testano solo i primi 5 
% test completi (setup richiesta singola)
%   con solo codice 
%   con desccrizione testuale del problema 
% test a chat
%   con descrizione solver
%   con descrizione solver e problema, dico che da qua in poi si testa solo sul migliore
% test con le features
%   Estrazione e spiegazione features
%   Risultati nei vari setup (feature da sole, con codice, con descrizioni testuali)

\section{Experiments}

In this section we are gonna show and explain al the experiments that led to the final choice of the context information and overall setup of the agentic solver.
\dots structure description \dots

\subsection{Preliminary tests}
\label{sec:initTests}
The first and most straightforward experiment was conducted using the raw problems. Each LLM was provided with the original MiniZinc model (\texttt{.mzn}) together with the corresponding instance data (in either \texttt{.dzn} or \texttt{.json} format), following the prompt structure defined in \Cref{sec:promptStruct}.

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.0-flash & 53.748 & 67 & 0.802 \\ 
            gemini & gemini-2.5-flash & 69.426 & 86 & 0.807 \\ 
            gemini & gemini-2.5-flash-lite & 69.040 & 85 & 0.812 \\ 
            gemini & gemini-2.5-pro & 41.594 & 51 & 0.815 \\ 
            groq & allam-2-7b & 0.0 & 10 & 0.0 \\ 
            groq & groq/compound & 8.246 & 9 & 0.916 \\ 
            groq & groq/compound-mini & 29.623 & 35 & 0.846 \\ 
            groq & llama-3.1-8b-instant & 56.228 & 72 & 0.780 \\ 
            groq & llama-3.3-70b-versatile & 9.496 & 10 & 0.949 \\ 
            groq & meta-llama/llama-4-maverick-17b-128e-instruct & 63.548 & 72 & 0.882 \\ 
            groq & meta-llama/llama-4-scout-17b-16e-instruct & 57.814 & 69 & 0.837 \\ 
            groq & meta-llama/llama-guard-4-12b & 0.0 & 77 & 0.0 \\ 
            groq & moonshotai/kimi-k2-instruct & 65.816 & 75 & 0.877 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 66.186 & 75 & 0.882 \\ 
            groq & openai/gpt-oss-120b & 64.508 & 74 & 0.871 \\ 
            groq & openai/gpt-oss-20b & 63.328 & 75 & 0.844 \\ 
            groq & qwen/qwen3-32b & 48.018 & 65 & 0.738 \\ \hline
        \end{tabular}
    }
    \label{tab:allLLM3}
    \caption{Initial tests giving plain scripts to all the LLMs, parallel-solver evaluation}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{allLLMspar.png}
    \caption{Scores obtained by all of the considered LLM from parallel-solver evaluation}
    \label{fig:allLLMpar}
\end{figure}

\begin{table}[h]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.5-flash-lite & 64.363 & 85 & 0.794 \\ 
            gemini & gemini-2.5-flash & 60.962 & 85 & 0.734 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 59.680 & 75 & 0.817 \\
            groq & moonshotai/kimi-k2-instruct & 58.609 & 75 & 0.837 \\ 
            groq & openai/gpt-oss-120b & 58.166 & 74 & 0.796 \\
            groq & openai/gpt-oss-20b & 57.154 & 75 & 0.828 \\
            groq & meta-llama/llama-4-maverick-17b-128e-instruct & 56.297 & 72 & 0.804 \\
            groq & meta-llama/llama-4-scout-17b-16e-instruct & 54.305 & 69 & 0.798 \\
            gemini & gemini-2.0-flash & 43.412 & 67 & 0.700 \\
            groq & qwen/qwen3-32b & 42.116 & 65 & 0.779 \\
            gemini & gemini-2.5-pro & 37.640 & 51 & 0.738 \\
            groq & llama-3.1-8b-instant & 36.082 & 72 & 0.546 \\ 
            groq & groq/compound-mini & 25.422 & 35 & 0.726 \\
            groq & llama-3.3-70b-versatile & 7.454 & 10 & 0.745 \\
            groq & groq/compound & 5.197 & 9 & 0.577 \\ 
            groq & allam-2-7b & 0.0 & 4 & 0.0 \\ \hline
        \end{tabular}%
    }
    \caption{Initial tests giving plain scripts to all the LLMs, single-solver evaluation}
    \label{tab:allLLM1}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{allLLMsingle.png}
    \caption{Scores obtained by all of the considered LLM from single-solver evaluation}
    \label{fig:allLLMsin}
\end{figure}

As reported in \Cref{fig:allLLMpar} and \Cref{tab:allLLM3} for parallel-solver evaluation, and in \Cref{tab:allLLM1} and \Cref{fig:allLLMsin} for single-solver evaluation, the resulting performance is generally poor for most models. While part of this outcome can be attributed to the deliberately simple formulation of the requests, a major limiting factor is the presence of strict rate limits, which prevent many instances from being processed by some, if not all, of the LLMs.

These constraints motivated both the adoption of script manipulation techniques, as described in \Cref{sec:expSetup}, and the decision to restrict subsequent experiments to the five best-performing LLMs identified in this preliminary phase, namely: \texttt{gpt-oss-120b}, \texttt{gemini-2,5-flash}, \texttt{gemini-2,5-flash-lite}, \texttt{kimi-k2-instruct-0905} and \texttt{kimi-k2-instruct}.

\subsection{Single Request Experiments}
After establishing a stable experimental pipeline, we repeated the evaluation on the full set of 100 instances. All experiments in this phase adopt a single-request setup, where each prompt is processed independently: the LLM receives the input and produces an answer without access to any prior interaction history or retained context.

\subsubsection{Base Setup}
As a baseline, the LLMs were first evaluated under the same conditions as the preliminary experiments, using only the raw MiniZinc and data scripts.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.5-flash & 79.105 & 100 & 0.791 \\ 
            gemini & gemini-2.5-flash-lite & 80.740 & 100 & 0.807 \\ 
            groq & moonshotai/kimi-k2-instruct & 83.268 & 100 & 0.832 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 82.656 & 100 & 0.826 \\ 
            groq & openai/gpt-oss-120b & 82.226 & 100 & 0.822 \\ \hline
        \end{tabular}%
    }
    \caption{Tests on sanitized scripts given to the 5 best performing LLMs, parallel-solver evaluation}
    \label{tab:base3}
\end{table}

As shown in \Cref{tab:base3}, performance in the parallel-solver evaluation is generally strong. All tested LLMs outperform every individual solver except \texttt{or-tools\_cp-sat-par}, which corresponds to the single best solver (SBS) in the open category and remains clearly dominant, with a substantial margin over both the LLM-based meta-solvers and the remaining individual solvers, as will be later reported in \Cref{tab:comparisanVarPar} and \Cref{fig:comparisanVarPar}.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            groq & openai/gpt-oss-120b & 74.488 & 100 & 0.744 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 71.622 & 100 & 0.753 \\ 
            groq & moonshotai/kimi-k2-instruct & 70.939 & 100 & 0.723 \\ 
            gemini & gemini-2.5-flash-lite & 70.145 & 100 & 0.738 \\ 
            gemini & gemini-2.5-flash & 69.763 & 98 & 0.742 \\ \hline
        \end{tabular}
    }
    \caption{Tests on sanitized scripts given to the 5 best performing LLMs, single-solver evaluation}
    \label{tab:base1}
\end{table}

Greater variability emerges in the single-solver evaluation (\Cref{tab:base1}). In this case, only \texttt{gpt-oss-120b} consistently outperforms all individual solvers other than the SBS in the free category (\texttt{or-tools\_cp-sat-free}), as will be later reported in \Cref{tab:comparisanVarSin} and \Cref{fig:comparisanVarSin}. The remaining LLMs still achieve competitive results compared to most standalone solvers, but a significant performance gap remains, as highlighted by the closed-gap scores reported in \Cref{tab:basecg}.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllllll}
        \hline
            Provider & Model & Instances Covered & AS & SBS & VBS & Closed Gap \\ \hline
            groq & openai/gpt-oss-120b & 100 & 74.488 & 76.964 & 89.0 & -0.205 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 100 & 71.622 & 76.964 & 89.0 & -0.443 \\ 
            groq & moonshotai/kimi-k2-instruct & 100 & 70.939 & 76.964 & 89.0 & -0.500 \\ 
            gemini & gemini-2.5-flash-lite & 100 & 70.145 & 76.964 & 89.0 & -0.566 \\ 
            gemini & gemini-2.5-flash & 98 & 69.763 & 76.964 & 89.0 & -0.598 \\ \hline
        \end{tabular}
    }
    \caption{Tests on sanitized scripts given to the 5 best performing LLMs, closed gap}
    \label{tab:basecg}
\end{table}

\subsubsection{Problem Description}
The results of the baseline experiments indicate that further improvements to the Agentic Solver are necessary. A natural approach is to provide the LLM with additional contextual information. However, as discussed previously, excessively large contexts can be counterproductive, potentially distracting the model and degrading performance rather than improving it\cite{ContextRot,LLMdistract}. This trade-off motivates a more careful investigation into which types of information are most beneficial for LLM-based solver selection.

As a first step, we augmented the prompt with a concise problem description (PD): a short textual summary of the MiniZinc model's semantics. These descriptions were automatically generated using another LLM (\texttt{GPT-5.1}) and subsequently refined manually to correct minor inaccuracies. The PD was incorporated into the prompt structure (\Cref{fig:promptEx}) as:
\[ \text{"Problem Description: "}+PD+PromptStructure \]
As shown in \Cref{tab:pDesc3} and \Cref{tab:pDesc1}, the inclusion of problem descriptions yields mixed effects. Single-solver performance consistently degrades, while parallel-solver evaluation shows modest improvements, with three out of the five tested LLMs achieving better results than in the base configuration. This divergence suggests that additional context can aid diversification in solver selection, even if, in this case, it does not reliably improve the choice of an optimal solver.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.5-flash & 59.154 & 75 & 0.788 \\ 
            gemini & gemini-2.5-flash-lite & 82.620 & 100 & 0.826 \\ 
            groq & moonshotai/kimi-k2-instruct & 81.889 & 100 & 0.818 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 83.182 & 100 & 0.831 \\ 
            groq & openai/gpt-oss-120b & 83.249 & 100 & 0.832 \\ \hline
        \end{tabular}
    }
    \caption{Test on sanitized scripts combined with textual problem description, parallel-solver evaluation}
    \label{tab:pDesc3}
\end{table}

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            groq & moonshotai/kimi-k2-instruct-0905 & 72.605 & 100 & 0.748 \\ 
            groq & openai/gpt-oss-120b & 70.740 & 100 & 0.721 \\ 
            gemini & gemini-2.5-flash-lite & 69.042 & 100 & 0.719 \\ 
            groq & moonshotai/kimi-k2-instruct & 67.554 & 100 & 0.718 \\ 
            gemini & gemini-2.5-flash & 49.896 & 73 & 0.723 \\ \hline
        \end{tabular}
    }
    \caption{Test on sanitized scripts combined with textual problem description, single-solver evaluation}
    \label{tab:pDesc1}
\end{table}

As can be deduced from the results, all the closed gap scores are negatives even after improving the context.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllllll}
        \hline
            Provider & Model & Instances Covered & AS & SBS & VBS & Closed Gap \\ \hline
            groq & moonshotai/kimi-k2-instruct-0905 & 100 & 72.605 & 76.964 & 89.0 & -0.362 \\ 
            groq & openai/gpt-oss-120b & 100 & 70.740 & 76.964 & 89.0 & -0.517 \\ 
            gemini & gemini-2.5-flash-lite & 100 & 69.042 & 76.964 & 89.0 & -0.658 \\ 
            groq & moonshotai/kimi-k2-instruct & 100 & 67.554 & 76.964 & 89.0 & -0.781 \\ 
            gemini & gemini-2.5-flash & 73 & 49.896 & 76.964 & 89.0 & -2.248 \\ \hline
        \end{tabular}
    }
    \caption{Test on sanitized scripts combined with textual problem description, closed gap}
    \label{tab:pDesccg}
\end{table}

\subsection{Multi-turn Experiments}
What we discussed so far indicate that the contextual information provided to the LLMs is either insufficient or, in some cases, not beneficial overall. A natural next step is therefore to explore alternative forms of context. However, this introduces two practical issues. First, the single-request setup already operates close to the maximum allowed tokens per minute (TPM), making it infeasible to simply add more information without violating rate limits. Second, the existing setup is inefficient in terms of token usage, as it repeatedly supplies redundant information.

More specifically, for each problem we evaluate five different instances, each with distinct data, while the underlying MiniZinc model and the associated problem description remain unchanged. Re-sending this invariant information with every request unnecessarily consumes tokens. Given the limited API resources available, addressing both constraints is essential. To this end, we transitioned to a multi-turn (chat-like) experimental setup.

\subsubsection{Setup Explanation}
The core idea of the multi-turn setup is to partition the interaction using role-based formatting~\cite{RBFormatting}. In the context of LLMs, messages are explicitly associated with roles,typically \texttt{system}, \texttt{user}, and \texttt{assistant}. Which helps the LLM distinguish between instructions, inputs, and generated outputs, while also maintaining conversational state across turns.

In our experiments, the \texttt{system} role is used to convey all invariant and high-level information, namely the MiniZinc model (\texttt{.mzn}) content, the textual descriptions, and the expected format of the answers. The \texttt{user} role is then reserved for instance-specific inputs, containing the data associated with each instance (in \texttt{.dzn} or \texttt{.json} format). This separation allows us to avoid repeatedly transmitting redundant context, significantly reducing token consumption per instance.

An additional advantage of the multi-turn setup is that it enables the handling of larger instance data by distributing content across multiple messages, while relying on the LLM's ability to retain previously supplied information within the same conversation. As a result, the system can accommodate longer and more complex inputs without exceeding rate limits.

\subsubsection{Solvers Description}
The increased efficiency of the multi-turn setup also makes it possible to enrich the contextual information with new textual data: solver descriptions. For each solver under consideration, a short textual description was generated and provided to the LLM within the \texttt{system} prompt, with the aim of improving the model's awareness of the available solver options and their respective characteristics.

To better understand the relative importance of this information, we experimented with two different configurations: one in which solver descriptions were combined with all previously provided contextual elements, and another in which they constituted the only additional textual information alongside the MiniZinc model and the instance data. This design allows us to assess the contribution of solver-specific knowledge to the overall performance of the Agentic Solver.

In the parallel-solver evaluation (\Cref{tab:sDesc3}), providing only solver descriptions does not lead to systematic improvements. The sole exception is \texttt{gemini-2.5-flash}, which shows a modest gain, although its performance remains well below that of the single best solver (SBS) in the open category.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.5-flash & 83.137 & 100 & 0.831 \\ 
            gemini & gemini-2.5-flash-lite & 77.746 & 100 & 0.777 \\ 
            groq & moonshotai/kimi-k2-instruct & 82.236 & 100 & 0.822 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 82.488 & 100 & 0.824 \\ 
            groq & openai/gpt-oss-120b & 78.799 & 100 & 0.787 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with solvers description in a multi turn setup, parallel-solver evaluation}
    \label{tab:sDesc3}
\end{table}

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            groq & moonshotai/kimi-k2-instruct & 76.964 & 100 & 0.769 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 76.964 & 100 & 0.769 \\ 
            gemni & gemini-2.5-flash & 71.686 & 100 & 0.716 \\ 
            groq & openai/gpt-oss-120b & 70.974 & 100 & 0.716 \\ 
            gemni & gemini-2.5-flash-lite & 54.713 & 100 & 0.552 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with solvers description in a multi turn setup, single-solver evaluation}
    \label{tab:sDesc1}
\end{table}

On the other hand, the effects are more pronounced in the single-solver evaluation, displayed in \Cref{tab:sDesc1}.\\ Two models, \texttt{moonshotai/kimi-k2-instruct-0905} and \texttt{moonshotai/kimi-k2-instruct}, exhibit a substantial apparent improvement, reaching the SBS score and thus achieving a closed-gap value of zero for the first time. A closer inspection of their outputs, however, reveals that this result is achieved by consistently selecting the same solver, namely \texttt{or-tools\_cp-sat-free}, which is itself the SBS. This behavior effectively bypasses the decision-making role of the LLM, thereby undermining the intended purpose of employing an LLM in the first place.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllllll}
        \hline
            Provider & Model & Instances Covered & AS & SBS & VBS & Closed Gap \\ \hline
            groq & moonshotai/kimi-k2-instruct & 100 & 76.964 & 76.964 & 89.0 & 0.0 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 100 & 76.964 & 76.964 & 89.0 & 0.0 \\ 
            gemini & gemini-2.5-flash & 100 & 71.686 & 76.964 & 89.0 & -0.438 \\ 
            groq & openai/gpt-oss-120b & 100 & 70.974 & 76.964 & 89.0 & -0.497 \\ 
            gemini & gemini-2.5-flash-lite & 100 & 54.713 & 76.964 & 89.0 & -1.848 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with solvers description in a multi turn setup, closed gap}
    \label{tab:sDesccg}
\end{table}

\subsubsection{Solver Description and Problem Description}
Following this observation, we evaluated a configuration combining both forms of textual context: solver descriptions and problem descriptions. This setup represents the maximum amount of contextual information provided to the LLMs yet.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            gemini & gemini-2.5-flash & 83.650 & 100 & 0.836 \\ 
            gemini & gemini-2.5-flash-lite & 78.147 & 100 & 0.781 \\ 
            groq & moonshotai/kimi-k2-instruct & 80.417 & 99 & 0.812 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 82.218 & 100 & 0.822 \\ 
            groq & openai/gpt-oss-120b & 80.295 & 100 & 0.802 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with both solvers description, and problem description in a multi turn setup, parallel-solver evaluation}
    \label{tab:sDescpDesc3}
\end{table}

In the parallel-solver evaluation, this configuration yields the highest scores observed so far, again driven primarily by \texttt{gemini-2.5-flash}. However, no meaningful improvements are observed for the remaining models, and overall performance still falls short of the open-category SBS.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllll}
        \hline
            Provider & Model & Total Score & Instances Covered & Average Score \\ \hline
            groq & openai/gpt-oss-120b & 77.260 & 100 & 0.780 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 76.964 & 100 & 0.769 \\ 
            groq & moonshotai/kimi-k2-instruct & 74.464 & 99 & 0.752 \\ 
            gemini & gemini-2.5-flash & 73.551 & 100 & 0.750 \\ 
            gemini & gemini-2.5-flash-lite & 63.062 & 100 & 0.630 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with both solvers description, and problem description in a multi turn setup, single-solver evaluation}
    \label{tab:sDescpDesc1}
\end{table}

In the single-solver evaluation, \texttt{moonshotai/kimi-k2-instruct-0905} continues to default to selecting the SBS exclusively. Nevertheless, improvements emerge for two other models, namely \texttt{gemini-2.5-flash} and \texttt{gpt-oss-120b}. In particular, \texttt{gpt-oss-120b} achieves the first strictly positive closed-gap score, surpassing \texttt{or-tools\_cp-sat-free} and marking a notable milestone in these experiments.

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lllllll}
        \hline
            Provider & Model & Instances Covered & AS & SBS & VBS & Closed Gap \\ \hline
            groq & openai/gpt-oss-120b & 100 & 77.260 & 76.964 & 89.0 & 0.024 \\ 
            groq & moonshotai/kimi-k2-instruct-0905 & 100 & 76.964 & 76.964 & 89.0 & 0.0 \\ 
            groq & moonshotai/kimi-k2-instruct & 99 & 74.464 & 76.964 & 89.0 & -0.207 \\ 
            gemini & gemini-2.5-flash & 100 & 73.551 & 76.964 & 89.0 & -0.283 \\ 
            gemini & gemini-2.5-flash-lite & 100 & 63.062 & 76.964 & 89.0 & -1.155 \\ \hline
        \end{tabular}
    }
    \caption{Test with sanitized scripts combined with both solvers description, and problem description in a multi turn setup, closed gap}
    \label{tab:sDescpDesccg}
\end{table}

\subsubsection{Basic Tests Evaluation}
Despite not meeting the most optimistic expectations, the results of this initial testing phase are nonetheless encouraging. The achievement of a positive closed gap in at least one configuration indicates that LLM-based agentic solvers can, under suitable conditions, outperform strong single solvers. Moreover, when we put the performance respect to the "non-best" solvers, these primitive configurations of agentic solvers still hold fairly competitive results.

To better display single LLMs performances, all variants scores are put together against one another, using histograms for better visualization. In \Cref{fig:allVarPar} for parallel-solver evaluation, \Cref{fig:allVarSin} for single-solver evaluation and \Cref{fig:allVarCg} for closed gap evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{allVarPar.png}
    \caption{Histogram to display all variants performances in parallel-solver evaluation}
    \label{fig:allVarPar}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{allVarSin.png}
    \caption{Histogram to display all variants performances in single-solver evaluation}
    \label{fig:allVarSin}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{allVarcg.png}
    \caption{Histogram to display all variants performances in closed gap evaluation}
    \label{fig:allVarCg}
\end{figure}

To give results another point of view, the performance of each agentic solver configuration is displayed against the single solvers of the corresponding category.
In \Cref{tab:comparisanVarPar} and \Cref{fig:comparisanVarPar} are shown the performance of all the configurations, scored with parallel-solvers evaluation, when put agains single solvers from the open category.
On the other hand, looking at \Cref{tab:comparisanVarSin} and \Cref{fig:comparisanVarSin}, the resulting score of single-solver evaluation of all the variants is put against all the single solvers from free category.

\begin{table}[!ht]
    \centering
    \resizebox{0.85\textwidth}{!}{
        \begin{tabular}{lll}
        \hline
            TotalScore & Solver & Type \\ \hline
            88.117 & or-tools\_cp-sat-par & Solver \\ 
            83.650 & gemini-2.5-flash (Top+SDesc+PDesc) & LLM  Variant \\ 
            83.268 & moonshotai/kimi-k2-instruct (Top) & LLM  Variant \\ 
            83.249 & openai/gpt-oss-120b (Top+Desc) & LLM  Variant \\ 
            83.182 & moonshotai/kimi-k2-instruct-0905 (Top+Desc) & LLM  Variant \\ 
            83.137 & gemini-2.5-flash (Top+SDesc) & LLM  Variant \\ 
            82.656 & moonshotai/kimi-k2-instruct-0905 (Top) & LLM  Variant \\ 
            82.620 & gemini-2.5-flash-lite (Top+Desc) & LLM  Variant \\ 
            82.488 & moonshotai/kimi-k2-instruct-0905 (Top+SDesc) & LLM  Variant \\ 
            82.236 & moonshotai/kimi-k2-instruct (Top+SDesc) & LLM  Variant \\ 
            82.226 & openai/gpt-oss-120b (Top) & LLM  Variant \\ 
            82.218 & moonshotai/kimi-k2-instruct-0905 (Top+SDesc+PDesc) & LLM  Variant \\ 
            81.889 & moonshotai/kimi-k2-instruct (Top+Desc) & LLM  Variant \\ 
            80.740 & gemini-2.5-flash-lite (Top) & LLM  Variant \\ 
            80.295 & openai/gpt-oss-120b (Top+SDesc+PDesc) & LLM  Variant \\ 
            79.105 & gemini-2.5-flash (Top) & LLM  Variant \\ 
            78.799 & openai/gpt-oss-120b (Top+SDesc) & LLM  Variant \\ 
            78.147 & gemini-2.5-flash-lite (Top+SDesc+PDesc) & LLM  Variant \\ 
            77.746 & gemini-2.5-flash-lite (Top+SDesc) & LLM  Variant \\ 
            74.819 & chuffed-free & Solver \\ 
            70.647 & picatsat-free & Solver \\ 
            68.784 & huub-free & Solver \\ 
            61.081 & gurobi-par & Solver \\ 
            60.301 & cplex-par & Solver \\ 
            59.365 & choco-solver\_\_cp\_-par & Solver \\ 
            58.216 & izplus-par & Solver \\ 
            57.681 & pumpkin-free & Solver \\ 
            56.896 & choco-solver\_\_cp-sat\_-par & Solver \\ 
            54.283 & gecode-par & Solver \\ 
            53.877 & cp\_optimizer-par & Solver \\ 
            47.304 & gecode\_dexter-open & Solver \\ 
            46.292 & or-tools\_cp-sat\_ls-par & Solver \\ 
            44.373 & jacop-free & Solver \\ 
            43.548 & sicstus\_prolog-free & Solver \\ 
            38.321 & yuck-par & Solver \\ 
            36.675 & scip-par & Solver \\ 
            33.598 & highs-par & Solver \\ 
            26.334 & cbc-par & Solver \\ 
            1.555 & atlantis-free & Solver \\ \hline
        \end{tabular}
    }
    \caption{Table displaying all the different LLM variants results from parallel-solver evaluation, and compared to all of the single solvers in open category of the MiniZinc Challenge\cite{MznResults25}}
    \label{tab:comparisanVarPar}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{comparison1stvarPar.png}
    \caption{Histograms to visualize the difference between LLMs and single solvers from open category performance}
    \label{fig:comparisanVarPar}
\end{figure}

\begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lll}
        \hline
            TotalScore & Solver & Type \\ \hline
            77.260 & openai/gpt-oss-120b (Solvers Description + Problem Description) & LLM Variant \\ 
            76.964 & moonshotai/kimi-k2-instruct-0905 (Solvers Description + Problem Description) & LLM Variant \\ 
            76.964 & moonshotai/kimi-k2-instruct-0905 (Solvers Description) & LLM Variant \\ 
            76.964 & moonshotai/kimi-k2-instruct (Solvers Description) & LLM Variant \\ 
            76.964 & or-tools\_cp-sat-free & Solver \\ 
            74.488 & openai/gpt-oss-120b (Simple) & LLM Variant \\ 
            74.464 & moonshotai/kimi-k2-instruct (Solvers Description + Problem Description) & LLM Variant \\ 
            74.456 & chuffed-free & Solver \\ 
            73.551 & gemini-2.5-flash (Solvers Description + Problem Description) & LLM Variant \\ 
            72.605 & moonshotai/kimi-k2-instruct-0905 (Problem Description) & LLM Variant \\ 
            71.686 & gemini-2.5-flash (Solvers Description) & LLM Variant \\ 
            71.622 & moonshotai/kimi-k2-instruct-0905 (Simple) & LLM Variant \\ 
            70.974 & openai/gpt-oss-120b (Solvers Description) & LLM Variant \\ 
            70.939 & moonshotai/kimi-k2-instruct (Simple) & LLM Variant \\ 
            70.933 & picatsat-free & Solver \\ 
            70.740 & openai/gpt-oss-120b (Problem Description) & LLM Variant \\ 
            70.145 & gemini-2.5-flash-lite (Simple) & LLM Variant \\ 
            69.763 & gemini-2.5-flash (Simple) & LLM Variant \\ 
            69.042 & gemini-2.5-flash-lite (Problem Description) & LLM Variant \\ 
            68.497 & huub-free & Solver \\ 
            67.554 & moonshotai/kimi-k2-instruct (Problem Description) & LLM Variant \\ 
            63.062 & gemini-2.5-flash-lite (Solvers Description + Problem Description) & LLM Variant \\ 
            60.808 & choco-solver\_\_cp-sat\_-free & Solver \\ 
            58.672 & izplus-free & Solver \\ 
            58.543 & pumpkin-free & Solver \\ 
            58.404 & choco-solver\_\_cp\_-free & Solver \\ 
            55.384 & gurobi-free & Solver \\ 
            54.713 & gemini-2.5-flash-lite (Solvers Description) & LLM Variant \\ 
            50.992 & cp\_optimizer-free & Solver \\ 
            50.073 & gecode-fd & Solver \\ 
            49.896 & gemini-2.5-flash (Problem Description) & LLM Variant \\ 
            48.514 & cplex-free & Solver \\ 
            44.592 & sicstus\_prolog-free & Solver \\ 
            44.549 & jacop-free & Solver \\ 
            43.222 & or-tools\_cp-sat\_ls-free & Solver \\ 
            36.902 & scip-free & Solver \\ 
            34.715 & yuck-free & Solver \\ 
            34.418 & highs-free & Solver \\ 
            22.615 & cbc-free & Solver \\ 
            1.5 & atlantis-free & Solver \\ \hline
        \end{tabular}
    }
    \caption{Table displaying all the different LLM variants results given single-solver evaluation, and compared to all of the single solvers in free category of the MiniZinc Challenge\cite{MznResults25}}
    \label{tab:comparisanVarSin}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{comparison1stvarSin.png}
    \caption{Histograms to visualize the difference between LLMs and single solvers from free category performance}
    \label{fig:comparisanVarSin}
\end{figure}


\end{document}